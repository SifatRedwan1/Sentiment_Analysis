# -*- coding: utf-8 -*-
"""Bangla_BERT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11xyWaRZOFgwO7L2wZtAbFlgLRuoSJbNe
"""


from transformers import AutoModelForPreTraining, AutoTokenizer
from normalizer import normalize # pip install git+https://github.com/csebuetnlp/normalizer
import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import AutoTokenizer
from torch.utils.data import DataLoader
from transformers import AutoModelForSequenceClassification
from transformers import AdamW
from torch.optim.lr_scheduler import StepLR
from torch.nn import CrossEntropyLoss
from sklearn.metrics import accuracy_score, f1_score, recall_score

model = AutoModelForPreTraining.from_pretrained("csebuetnlp/banglabert_small")
tokenizer = AutoTokenizer.from_pretrained("csebuetnlp/banglabert_small")

# Load the CSV files
train_data = pd.read_csv("Train.csv")
test_data = pd.read_csv("Test.csv")

# Ensure your data has 'text' and 'label' columns
# If not, rename columns accordingly
train_data = train_data.rename(columns={"Data": "text", "Label": "label"})
test_data = test_data.rename(columns={"Data": "text", "Label": "label"})

# Convert pandas DataFrame to Hugging Face Dataset
train_dataset = Dataset.from_pandas(train_data)
test_dataset = Dataset.from_pandas(test_data)

# Print to verify
print(train_dataset)
print(test_dataset)

# Tokenize function
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

# Apply tokenizer
train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Set format for PyTorch
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)
test_dataloader = DataLoader(test_dataset, batch_size=16)

optimizer = AdamW(model.parameters(), lr=5e-5)

# Learning rate scheduler
scheduler = StepLR(optimizer, step_size=2, gamma=0.1)

# Set device
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

loss_fn = CrossEntropyLoss()

# Training loop
epochs = 8
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in train_dataloader:
        optimizer.zero_grad()

        # Move data to device
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        # Forward pass
        outputs = model(input_ids, attention_mask=attention_mask)
        loss = loss_fn(outputs.logits, labels)


        # Backward pass
        loss.backward()
        optimizer.step()
    scheduler.step()
    print(f"Epoch {epoch + 1} - Loss: {total_loss / len(train_dataloader)}")